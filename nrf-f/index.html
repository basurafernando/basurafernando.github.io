<html><head>
<meta http-equiv="CACHE-CONTROL" content="NO-CACHE">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="LAHA">

<title>Learning why do we do what we do - Understanding of human actions using AI </title>
 <script language="JavaScript">
        function ShowHide(divId)
        {
            if(document.getElementById(divId).style.display == 'none')
            {
                document.getElementById(divId).style.display='block';
            }
            else
            {
                document.getElementById(divId).style.display = 'none';
            }
        }
		
		function showaboutMeDisplay() {
				if (document.getElementById("divabout").style.display == "none")
				{
					document.getElementById("divabout").style.display = "block";
				}
				else
				{
					document.getElementById("divabout").style.display = "none";
				}
		}
		
		function showAbstract(divId)
		{
			if(document.getElementById(divId).style.display == 'none')
            {
                document.getElementById(divId).style.display='block';
            }
            else
            {
                document.getElementById(divId).style.display = 'none';
            }
		}
		
        </script>

<style type="text/css">
body {
	font-family:verdana,arial,sans-serif;
	font-size:10pt;
	margin:30px;
	background-color:#FFFFFF;
	}
	
	span.titleSt {
	font-size:14pt;
	color: #0000FF;
	}
	
	span.author {
	font-size:10pt;
	color: #000000;
	}
	
	span.conf {
		font-size:11pt;
		color: #111;
		font-weight:bold;
	}
	span.PDF {
		font-size:13pt;
		color: #555;
	}
	span.Note {
		font-size:13pt;
		color: #900;
	}
	
	
div.ex
{
width:800px;
padding:1px;
border:0px solid gray;
margin:0px;
}

div.extra
{
width:1000px;
padding:1px;
border:0px solid gray;
margin:0px;
}

div.bib{
        margin:15px;
		border:2px solid #a1a1a1;
		padding:5px 10px; 
		background:#dddddd;
		width:400px;
		border-radius:15px;
		font-size:80%;
}

div.abstract{
        margin:15px;
		border:2px solid #a1a1a1;
		padding:5px 10px; 
		background:#dddddd;
		width:600px;
		border-radius:15px;
		font-size:80%;
}

img{height: 100; width: 170px;image-rendering: smooth;}

tr.spaceUnder > td
{
  padding-bottom: 2em;
}

tr.spaceUnder2 > td
{
  padding-bottom: 3em;
}

span.year {
		font-size:14pt;
		color: #247;
		font-weight:bold;
		text-decoration: underline;
	}

/*Button Style*/ .button { float:left; height:auto; font:96%/150% "Lucida Grande", Geneva, Verdana, Arial, Helvetica, sans-serif; width:12em; text-align:center; white-space:nowrap; } 
/*Button Arrow Styles*/ .arrows { font-size:90%; margin:0.2em; } 
/*Button link styles*/ .button a:link, .button a:visited { color: #000; background-color:#eee; font-size:1em; font-weight:bolder; text-decoration: none; border-bottom:0.1em solid #555; border-right:0.1em solid #444; border-top:0.1em solid #ccc; border-left:0.1em solid #ccc; margin: 0.2em; padding:0.2em; display:block; } 
.button a:hover { background-color:#aaa; color:#fff; border-top:0.1em solid #777; border-left:0.1em solid #777; border-bottom:0.1em solid #aaa; border-right:0.1em solid #aaa; padding:0.2em; margin: 0.2em; }

a:visited {
    color:#0000FF;
}
	
</style>
</head>
<body marginwidth="0" marginheight="0">

<h1><a href="#" >Learning why do we do what we do - Understanding human actions using Neurosymbolic AI</a></h1>
<h3>This research is supported by the National Research Foundation Fellowship. Duration : April 2022 to March 2027.</h3>

<img style="height: 400px; width: 1000px"  src="nrf.png">



<div id="divabout" class="extra" >
<p>Recognizing what we do or what we will do has been well investigated under action recognition and anticipation literature in the Computer Vision (CV) and Machine Learning (ML) research communities. However, computational learning of why do we do what we do is not well investigated. The objective of this project is to develop Artificial Intelligent (AI) models to process videos and learn why do humans do what they do? by reducing the gap between neural and symbolic representation through novel neurosymbolic AI. These neurosymbolic AI models can see what we do and then reason about our behavior to interpret, justify, explain and understand our actions. </p> 



<a  href="https://github.com/LUNAProject22/" target="_blank" style="font-size:13pt;color: #117;"> Codes are also available here!</a>


<h2>Publications </h2>

<table>

<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2025_IJCV.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Effectively Leveraging CLIP for Generating Situational Summaries of Images and Videos
	</span></br>
	<span class="author">
Dhruv Verma, Debaditya Roy, Basura Fernando
</span></br>	
	<span class="conf">International Journal of Computer Vision - IJCV 2025 (Accepted)</span></br>   		
	<span class="PDF"><a  href="https://arxiv.org/abs/2407.20642" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>	
	<span class="PDF"><a  href="https://github.com/LUNAProject22/CLIPSitu-video" target="_blank" style="font-size:13pt;color: #555;"> Code</a>	
	
	<span class="PDF">							
	</span></br>	
	</div>
</td>
</tr>

<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2025_KML.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering
	</span></br>
	<span class="author">
Thanh-Son Nguyen, Hong Yang, Tzeh Yuan Neoh, Hao Zhang, Ee Yeo Keat, Basura Fernando
</span></br>	
	<span class="conf">Preprint</span></br>   		
	<span class="PDF"><a  href="https://arxiv.org/abs/2503.14957" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>	
	<span class="PDF">							
	</span></br>	
	</div>
</td>
</tr>




<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2025_ACL.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning
	</span></br>
	<span class="author">
Xinyu Zhang, Yuxuan Dong, Yanrui Wu, Jiaxing Huang, Chengyou Jia, Basura Fernando, Mike Zheng Shou, Lingling Zhang, Jun Liu
</span></br>	
	<span class="conf">Preprint</span></br>   		
	<span class="PDF"><a  href="https://arxiv.org/abs/2502.12054" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	
	<span class="PDF">							
	</span></br>	
	</div>
</td>
</tr>


<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2025_WACV_CATE.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Learning to Visually Connect Actions and their Effects
	</span></br>
	<span class="author">
Paritosh Parmar and Eric Peh and Basura Fernando
</span></br>	
	<span class="conf">WACV 2025</span></br>   		
	<span class="PDF"><a  href="https://arxiv.org/pdf/2401.10805" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	
	<span class="PDF"><a  href="https://github.com/LUNAProject22/CATE" target="_blank" style="font-size:13pt;color: #555;"> code</a>							
	</span></br>	
	</div>
</td>
</tr>



<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2025_WACV_AAR.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Inferring Past Human Actions in Homes with Abductive Reasoning
	</span></br>
	<span class="author">
Clement Tan Son and Chai Kiat Yeo and Cheston Tan and Basura Fernando
</span></br>	
	<span class="conf">WACV 2025</span></br>   		
	<span class="PDF"><a  href="https://basurafernando.github.io/papers/2025_WACV_AAR.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<span class="PDF"><a  href="https://github.com/LUNAProject22/AAR" target="_blank" style="font-size:13pt;color: #555;"> code</a>							
	</span></br>	
	</div>
</td>
</tr>




<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2025_WACV_SRD.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Effective Scene Graph Generation by Statistical Relation Distillation
	</span></br>
	<span class="author">
Nguyen Thanh Son and Hong Yang and Basura Fernando
</span></br>	
	<span class="conf">WACV 2025</span></br>   		
	<span class="PDF"><a  href="https://basurafernando.github.io/papers/2025_WACV_SRD.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<span class="PDF"><a  href="https://github.com/LUNAProject22/SRD" target="_blank" style="font-size:13pt;color: #555;"> code</a>							
	</span></br>	
	</div>
</td>
</tr>


<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2025_WACV_SSG.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Situational Scene Graph for Structured Human-centric Situation Understanding
	</span></br>
	<span class="author">
Chinthani Sugandhika and Chen Li  and Deepu Rajan and Basura Fernando
</span></br>	
	<span class="conf">WACV 2025</span></br>   		
	<span class="PDF"><a  href="https://basurafernando.github.io/papers/2025_WACV_SSG.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<span class="PDF"><a  href="https://github.com/LUNAProject22/SSG" target="_blank" style="font-size:13pt;color: #555;"> code</a>							
	</span></br>	
	</div>
</td>
</tr>


<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2025_WACV_VITTIFF.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Deduce and Select Evidences with Language Models for Training-Free Video Goal Inference
	</span></br>
	<span class="author">
Yeo Keat Ee and Hao Zhang and Alexander Matyasko and Basura Fernando
</span></br>	
	<span class="conf">WACV 2025</span></br>   		
	<span class="PDF"><a  href="https://basurafernando.github.io/papers/2025_WACV_VITTIFF.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<span class="PDF"><a  href="https://github.com/LUNAProject22/VITTIFF" target="_blank" style="font-size:13pt;color: #555;"> code</a>							
	</span></br>	
	</div>
</td>
</tr>




<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2024_Neurips_CausalChaos.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes
	</span></br>
	<span class="author">
Paritosh Parmar, Eric Peh, Ruirui Chen, Ting En Lam, Yuhan Chen, Elston Tan, Basura Fernando
</span></br>	
	<span class="conf">NeurIPS 2024 (Accepted)</span></br>   		
	<span class="PDF"><a  href="https://arxiv.org/abs/2404.01299" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<span class="PDF"><a  href="https://github.com/LUNAProject22/CausalChaos" target="_blank" style="font-size:13pt;color: #555;"> code</a>							
	</span></br>	
	</div>
</td>
</tr>



<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2024_Neurips_IPRM.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios
	</span></br>
	<span class="author">
Shantanu Jaiswal, Debaditya Roy, Basura Fernando, Cheston Tan
</span></br>	
	<span class="conf">NeurIPS 2024 (Accepted)</span></br>   		
	<span class="PDF"><a  href="https://basurafernando.github.io/papers/2024_Neurips_IPRM.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<span class="PDF"><a  href="" target="_blank" style="font-size:13pt;color: #555;"> code (soon)</a>							
	</span></br>	
	</div>
</td>
</tr>



<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2024_ACMMM.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
RCA: Region Conditioned Adaptation for Visual Abductive Reasoning
	</span></br>
	<span class="author">
Hao Zhang and Yeo Keat Ee and Basura Fernando</span></br>	
	<span class="conf">ACM MM (Accepted 2024)</span></br>   		
	<span class="PDF"><a  href="https://basurafernando.github.io/papers/2024_ACMMM.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<span class="PDF"><a  href="https://github.com/LUNAProject22/RPA/" target="_blank" style="font-size:13pt;color: #555;"> code</a>							
	</span></br>	
	</div>
</td>
</tr>


<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2024_ICPR.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Predicting the Next Action by Modeling the Abstract Goal
	</span></br>
	<span class="author">
Debaditya Roy and Basura Fernando</span></br>	
	<span class="conf">ICPR (Accepted 2024)</span></br>   		
	<span class="PDF"><a  href="https://basurafernando.github.io/papers/2024_ICPR.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<span class="PDF"><a  href="https://github.com/LUNAProject22/" target="_blank" style="font-size:13pt;color: #555;"> code</a>						
	
	</span></br>	
	</div>
</td>
</tr>


<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2024_ICML.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion
	</span></br>
	<span class="author">
Ishaan Singh Rawal, Alexander Matyasko, Shantanu Jaiswal, Basura Fernando, Cheston Tan</span></br>	
	<span class="conf">ICML (Accepted 2024)</span></br>   		
	<span class="PDF"><a  href="https://basurafernando.github.io/papers/2024_ICML.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<span class="PDF"><a  href="https://dissect-videoqa.github.io/" target="_blank" style="font-size:13pt;color: #555;"> code</a>						
	
	</span></br>	
	</div>
</td>
</tr>

<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2023_ICCV_CIN.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Who are you referring to? Coreference resolution in image narrations
	</span></br>
	<span class="author">
Arushi Goel and Basura Fernando and Frank Keller and Hakan Bilen</span></br>		
<span class="conf">International Conference on Computer Vision - ICCV (2023)</span></br>   		

	<span class="PDF">
	<a  href="https://arxiv.org/pdf/2211.14563.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>
	<a  href="https://groups.inf.ed.ac.uk/vico/research/CIN/" target="_blank" style="font-size:13pt;color: #555;"> CIN Dataset</a>
	<a  href="https://github.com/VICO-UoE/CIN" target="_blank" style="font-size:13pt;color: #555;"> Code</a>		
	<a onclick="javascript:ShowHide(&#39;arushi2023&#39;)" href="javascript:;" style="font-size:13pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="arushi2023" style="display: none;">
@inproceedings{arushi2023,
  title={Who are you referring to? Coreference resolution in image narrations},
  author={Arushi Goel and Basura Fernando and Frank Keller and Hakan Bilen},
  booktitle={International Conference on Computer Vision 2023},
  pages={},
  year={2023}
}
</br>
</br>
<button class="close" onclick="document.getElementById('arushi2023').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>	
	</span></br>	
	</div>
</td>
</tr>


<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2023_EMNLP.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Semi-supervised multimodal coreference resolution in image narrations
	</span></br>
	<span class="author">
Arushi Goel and Basura Fernando and Frank Keller and Hakan Bilen</span></br>		
<span class="conf">Empirical Methods in Natural Language Processing - EMNLP (2023)</span></br>   		

	<span class="PDF">
	<a  href="https://arxiv.org/pdf/2310.13619.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>	
	<a onclick="javascript:ShowHide(&#39;emnlp2023&#39;)" href="javascript:;" style="font-size:13pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="emnlp2023" style="display: none;">
@inproceedings{emnlp2023,
  title={Semi-supervised multimodal coreference resolution in image narrations},
  author={Arushi Goel and Basura Fernando and Frank Keller and Hakan Bilen},
  booktitle={Empirical Methods in Natural Language Processing 2023},
  pages={},
  year={2023}
}
</br>
</br>
<button class="close" onclick="document.getElementById('emnlp2023').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>	
	</span></br>	
	</div>
</td>
</tr>


<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2023_ICCV_DA.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Energy-based Self-Training and Normalization for Unsupervised Domain Adaptation
	</span></br>
	<span class="author">
Samitha Herath, Basura Fernando, Ehsan Abbasnejad, Munawar Hayat, Shahram Khadivi, Mehrtash Harandi, Hamid Rezatofighi, and Reza Haffari </span></br>		
<span class="conf">International Conference on Computer Vision  - ICCV (2023)</span></br>   		

	<span class="PDF">
	<a  href="https://basurafernando.github.io/papers/2023_ICCV_DA.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>	
	<a onclick="javascript:ShowHide(&#39;samith2023&#39;)" href="javascript:;" style="font-size:13pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="samith2023" style="display: none;">
@inproceedings{samith2023,
  title={Energy-based Self-Training and Normalization for Unsupervised Domain Adaptation},
  author={Samitha Herath and Basura Fernando and Ehsan Abbasnejad and Munawar Hayat and Shahram Khadivi and Mehrtash Harandi and Hamid Rezatofighi and Reza Haffari},
  booktitle={International Conference on Computer Vision 2023},
  pages={},
  year={2023}
}
</br>
</br>
<button class="close" onclick="document.getElementById('samith2023').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>	
	</span></br>	
	</div>
</td>
</tr>


<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/2023_WACV_CLIPSITU.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition
	</span></br>
	<span class="author">
Debaditya Roy and Dhruv Verma and Basura Fernando</span></br>		
<span class="conf">IEEE/CVF Winter Conference on Applications of Computer Vision - WACV (2024)</span></br>   	
<span class="conf"><a href="https://paperswithcode.com/sota/grounded-situation-recognition-on-swig">Best results in SWiG - 2024</a></span></br> 	
<span class="conf"><a href="https://paperswithcode.com/sota/situation-recognition-on-imsitu">Best results in imSitu - 2024</a></span></br> 	
<a  href="https://github.com/LUNAProject22/CLIPSitu" target="_blank" style="font-size:13pt;color: #555;"> Code</a>		

	<span class="PDF">
	<a  href="https://basurafernando.github.io/papers/2023_WACV_CLIPSITU.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>	
	<a onclick="javascript:ShowHide(&#39;clipsitu2023&#39;)" href="javascript:;" style="font-size:13pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="clipsitu2023" style="display: none;">
@inproceedings{clipsitu2024,
  title={ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition},
  author={Debaditya Roy and Dhruv Verma and Basura Fernando},
  booktitle={IEEE/CVF Winter Conference on Applications of Computer Vision WACV 2024},
  pages={},
  year={2024}
}
</br>
</br>
<button class="close" onclick="document.getElementById('clipsitu2023').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>	
	</span></br>	
	</div>
</td>
</tr>


<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/region_promt.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
A Region-Prompted Adapter Tuning for Visual Abductive Reasoning
	</span></br>
	<span class="author">
Hao Zhang, Yeo Keat Ee, Basura Fernando</span></br>
	<span class="conf">Preprint</span></br>   		
	<span class="PDF">
	<a  href="https://arxiv.org/abs/2303.10428" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>		
</td>
</tr>

<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/abductiveaction.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Abductive Action Inference
	</span></br>
	<span class="author">
Clement Tan and Chai Kiat Yeo and Cheston Tan and Basura Fernando</span></br>
	<span class="conf">Preprint</span></br>   		
	<span class="PDF">
	<a  href="https://arxiv.org/pdf/2210.13984.pdf" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>	            	
	</span></br>	
	</div>
</td>
</tr>

<tr class="spaceUnder"><td  valign="top"><img src="https://basurafernando.github.io/images/actioneffect.png"></td>
<td valign="top">
	<div class="ex">
	<span class="titleSt">
Learning to Visually Connect Actions and their Effects
	</span></br>
	<span class="author">
Eric Peh and Paritosh Parmar and Basura Fernando</span></br>
	<span class="conf">Preprint</span></br>   		
	<span class="PDF">
	<a  href="https://arxiv.org/pdf/2401.10805" target="_blank" style="font-size:13pt;color: #555;"> PDF</a>	            	
	</span></br>	
	</div>
</td>
</tr>



</table>
</div>


<a name="master" > <h2>Past and Current Team Members</h2></a>

<p style="font-size:14pt;color: #555;">  <a target="_blank"  href="https://scholar.google.com.sg/citations?user=6_rJ2pcAAAAJ&hl=en">Dr. Chen Li</a> (Research Scientist)</br></p>
<p style="font-size:14pt;color: #555;">  <a target="_blank"  href="https://scholar.google.co.in/citations?user=npektxAAAAAJ&hl=en">Dr. Paritosh Parmar</a> (Research Scientist)</br></p>
<p style="font-size:14pt;color: #555;">  <a target="_blank"  href="https://scholar.google.com/citations?user=Cqy13vYAAAAJ&hl">Dr. Hao Zhang</a> (Research Scientist)</br></p>
<p style="font-size:14pt;color: #555;"> <a target="_blank"  href="https://scholar.google.com.sg/citations?hl=en&user=OgPcbjgAAAAJ&view_op=list_works&sortby=pubdate">Dr. Thanh-Son Nguyen</a> (Research Scientist) </br></p>
<p style="font-size:14pt;color: #555;">  <a target="_blank"  href="https://scholar.google.com/citations?user=9GDRHp8AAAAJ&hl=en">Dr. Alexander Matyasko</a> (Research Scientist)</br></p>
<p style="font-size:14pt;color: #555;"> <a target="_blank"  href="https://sites.google.com/view/debadityaroy/home">Dr. Debaditya Roy</a> (Research Scientist)</br></p>
<p style="font-size:14pt;color: #555;">  <a target="_blank"  href="">Eric Peh</a> (Research Engineer)</br></p>
<p style="font-size:14pt;color: #555;">  <a target="_blank"  href="">Dhruv Verma</a> (Research Engineer)</br></p>
<p style="font-size:14pt;color: #555;">  <a target="_blank"  href="">Ee Yeo Keat</a> (Research Engineer)</br></p>
<p style="font-size:14pt;color: #555;">  <a target="_blank"  href="https://scholar.google.com/citations?user=T6ZP9u4AAAAJ&hl=en">Yang Hong</a> (Research Engineer) </br> </p>
<p style="font-size:14pt;color: #555;"> <a target="_blank"  href="https://dblp.uni-trier.de/pers/hd/j/Jaiswal:Shantanu">Jaiswal Shantanu</a> (Research Engineer) </br></p>


<p style="font-size:14pt;color: #555;">PhD student - <a target="_blank" href=""> Clement Tan </a> (NTU) </br> </p>
<p style="font-size:14pt;color: #555;">PhD student - <a target="_blank" href=""> BURTON-BARR JONATHAN WESTON </a> (NTU) </br> </p>
<p style="font-size:14pt;color: #555;">PhD student - <a target="_blank" href=""> Chinthani Sugandhika </a> (NTU) </br> </p>







</html>

